\subsection{Motivation(1.5pg)}
\section{Intro}
Program consume memory. Program might use too much memory:
\begin{itemize}
	\item The input might be huge. Editing a file require loading the whole file into memory; Autocomplete load and parse the whole codebase; Image editors will keep multiple layers of an image, each consuming as much space as the raw image itself.
	\item Exploding intermediate state: Model checker and Chess bot search through a huge state space, Compiler and Interprocedural analysis expand the AST, Deep Learning/Backpropagation create and keep intermediate value. Additionally, interactive/incrmental application such as Juypter Notebook, Undo/Redo Stack, Time travelling debugger, program slicing, also keep intermediate state around to revert back to an older state when needed.
	\item Low memory limit. wasm have a memory limit of 2GB, embedded device or poor people may have device with lower ram. A personal computer might be running multiple programs, which must contend for a fixed memory limit. GPU and other accelerator use their own separate memory and usually is of smaller capacity then main memory.
\end{itemize}

Even when there is enough memory, it is still beneficial to use less memory, as multi-tenancy is ubiquitous. Operating System use memory for file cache, the program may have software cache, garbage collector fire less often give more free space, and extra space could be used to increase load or to execute other process.

One common-bought up solution to the memory problem is - to just buy more memory. However this only work to certain extents. Accelerators and wasm have a fixed memory limit and memory-limited algorithm can improve space consumption asymptotically, a feat that merely hardware improvement cannot catch up to. \todo{should i talk about banker paradox here?}

However, the only generic technique for reducing memory consumption, swapping, come with great drawback. Swapping move a chunk of data from main memory onto disk, and upon re-requesting it, move the data from disk back into main memory. It degrade performance significantly as disk is far slower then CPU and main memory, and it is especially bad for functional programs as most values is functional programs are boxed, they typically have less cache locality, exacerbated by swapping which operate on pages granularity.

Facing the lack of a generic solution, developers resort to ad-hoc, case-by-case solution, such as cache size tuning, reducing heap size, creating algorithm that swap and uncompute. Such solution complicate the system, and as the developer is likely not an expert on memory-constrained solution, suboptimal design/implementation is often made.

We propose uncomputation, a technique to reduce memory consumption for arbitrary purely functional programs. When needed, uncomputation select some values in the object graph according to cache policy, reverting it back into thunk, releasing memory until it is needed again, in which case it will recompute said values and replacing the thunk with the values. We provided a monadic API behind uncomputation, such that mere mechnical translation is needed to apply our work. The monadic API also grant compatibility, saving developers headache from learning and converting between a full-fledge language.

\subsection{Guarantee(0.5pg)}
There are multiple guarantees our approach give.

Correctness guarantee: uncomputing will not effect the final result (preservation), and will not be in an infinite loop (progress). Essentially, uncomputing is transparent: the semantic is that of the same as if none happens at all.

Asymptotic guarantee: O(1) access time for all in-memory objects, and O(1) space for each in-memory objects. In particular, this imply that we are not using any space for uncomputed objects.

\subsection{Insight}
While the above asymptotic guarantee seems very strong, it is actually achievable via:

\begin{itemize}
	\item Record undelimited context instead of closures. Undelimited context, when run long enough, will produce everything later.
	\item Assigning a monotonically increasing number, tock, to all zombie creation, and to all call/return/tailcall. This give each zombie a global unique id, so repeat creation of zombie do not create duplicate objects. It also give an ordering so we can find earlier context to replay.
	\item A data structure, tock tree. The tock tree is a binary search tree, that, on lookup failure, instead of failing (via exception/nullptr/optional), will return the closest match: the value associated with the largest key less then/equal to the input key. This design allow removal of arbitrary context, except the earliest one, as removing an earlier context merely cause lookup to return a further context, requiring more replay.
\end{itemize}

\pavel I hate how we talk bad about a naive approach later, but explain our core idea here. I think we should not have naive approach at all, or have it before core idea. Also - 1.3 and 1.4 is new. What other thing do I need for the intro?