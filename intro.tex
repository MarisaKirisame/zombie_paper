\subsection{Motivation(1.5pg)}
\section{Intro}
Program consume memory. Program might use too much memory:
\begin{itemize}
	\item The input might be huge. Editing a file require loading the whole file into memory; Autocomplete load and parse the whole codebase; Image editors will keep multiple replica of similar images.
	\item Exploding intermediate state: Model checker and Chess bot search through a huge state space, Compiler and Interprocedural analysis expand the AST, Deep Learning/Backpropagation create and keep intermediate value. Additionally, interactive/incrmental application such as Juypter Notebook, Undo/Redo Stack, Time travelling debugger also keep intermediate state around to revert back to an older state when needed.
	\item Low memory limit. wasm have a memory limit of 2GB, embedded device or poor people may have device with lower ram. A personal computer might be running multiple programs, which must contend for a fixed memory limit. GPU and other accelerator use their own separate memory and usually is of smaller capacity then main memory.
\end{itemize}

Even when there is enough memory, it is still beneficial to use less memory. Operating System use memory for file cache, the program may have software cache, garbage collector fire less often give more free space, and extra space could be used to increase load or to execute other process.

One common-bought up solution to the memory problem is - to just buy more memory. However this only work to certain extents. Accelerators and wasm have a fixed memory limit and memory-limited algorithm can improve space consumption asymptotically, a feat that merely hardware improvement cannot catch up to. \todo{should i talk about banker paradox here?}

However, the only generic technique for reducing memory consumption, swapping, come with great drawback. Swapping move a chunk of data from main memory onto disk, and upon re-requesting it, move the data from disk back into main memory. It degrade performance significantly as disk is far slower then CPU and main memory, and it is especially bad for functional programs as most values is functional programs are boxed, they typically have less cache locality, exacerbated by swapping which operate on pages granularity.

We propose uncomputation, a technique to reduce memory consumption for arbitrary purely functional programs. When needed, uncomputation select some values in the object graph according to cache policy, reverting it back into thunk, releasing memory until it is needed again, in which case it will recompute said values and replacing the thunk with the values.

\subsection{Pebble game}
The idea of uncomputation is not new. More specifically, uncomputation had been studied rather heavily by the TCS community, by modelling it as a 'pebble game'. 

Pebble game is played on a computation graph, a directed acyclic graph where vertex represent data and edge represent computational dependency between data. The game is played with a fixed amount of identical pebble, representing memory, with each vertex possibly having one pebble on it. 

It is played with the following rules:
\begin{itemize}
\item If a vertex have no pebble, but all it's parent have pebble, a pebble can be placed on it. This represent computing a value based on it's dependency, and stroing the result.
\item A pebble can be removed from a vertex. This represent deallocating a value.
\item The game is finished if a pebble is placed on a special vertex, the result.
\end{itemize}

The aim of the game is to finish it in the minimal amount of compute step.

The pebble game is a prolific area of TCS research, with many variation of pebble game, modeling swapping or partially reversible computation, or with different cost/constraint. Special casing on the computation graph, where it form a planar graph or a tree, had likewised be studied. However, in the general case, on the classical pebble game is very diffcult.

It is hard to use less pebbles. John Hopcrof prove that for a pebble game of size X, one can use X / log(X) amount of pebble, but in general, no less. \todo{recheck this no less claim}. Moreover given a computation graph, the minimal amount of pebble (alongside the strategy) is not only NP-complete, but also inapproximable.

\todo{what is the transition here?}
\subsection{Guarantee(0.5pg)}
There are multiple guarantees our approach give.

Correctness guarantee: uncomputing will not effect the final result (preservation), and will not be in an infinite loop (progress)

Asymptotic guarantee: O(1) access time for all in-memory objects, and O(1) space for each in-memory objects. In particular, this imply that we are not using any space for uncomputed objects.

\subsection{Key Idea(1pg)}
\todo{I dont like this section. feels like spoiler.}
There are 3 key insights that enable the above guarantee:
\begin{itemize}
	\item Use a monadic API \todo{this is a bit week but is a dependency}. A monadic API give a generic yet turing-complete \todo{better word} method of extending a language. Furthermore it immediately ask an interesting question: what is the meaning of Zombie<Zombie<X>>? likewise, what is join?
	\item A global clock increasing on return/bind invocation. This not only give a way to quickly detect same objects for hashconsing, but as opposed to classical hash consing, the key itself have semantic meaning: < denote happens-before.
	\item Recomputing A also recompute what is inside A. This mean we can forget about some inner thunk, as long as we can redirect the pointers to such thunk to an outer thunk. One possibility is to track backpointers and fix them, but this take significantly more time and space. Instead, Track function exit time too. Entering and exiting form a range, and two separate range either form a nested relationship or disjoint relationship. We can then design a datastructure, such that lookup failure return a match one-level above.
\end{itemize}

\todo{this is old tex. merge.}

There are multiple insights that allow us to solve the above 4 problems. Explaining them aid understanding the implementation of Zombie.

0: treat each cell as independent, ignoring all member relationship.
this trivialize size finding.

Zombie allow recursive object, e.g. a list, a tree, or a graph.
For those object, a part of the object can be evicted, and sharing does not affect size counting.
This feature is implemented by ignoring recursiveness. All nesting need to loop through Zombie, and when it does that, the management of subsequent nested Zombie is completely handoff to it.
Note that this allow keeping some value of the list in memory, even when there is previous node evicted!

1: hashconsing. to avoid the same expression producing multiple values, we introduce a type Idx, such that the same trace will be tagged with the same Idx.
Zombie X hold Idx instead of X, and the actual values are stored in a datastructure.
When return x try to create a Zombie X of tag idx, if idx is stored in the datastructure, that value is used instead, even though the computed value is equivalent to said value. This cause x to be immediately unreachable and can be released from memory. This allow evicting most of the list, only keeping some intermediate value. nowhere reachable via the object graph.

2: note that bind can be nested: the function in bind may call more bind. When such nesting occur, executing the higher-level bind will also re-invoke the lower level bind. Hence - it is not necessary to store the lower-level thunk (even though doing so improve performance, as it will execute less code).

With this in mind, if we can create a data structure imitating the bind/return execution of the program, we can remove non top-level nodes, so that lookup will return a higher-level result then the remove node, we solved the breadcrumb problem.