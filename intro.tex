% \subsection{Motivation}
% Program execution consume both space and time. While there are tons of research focused on reducing the time spent running a program, memory usage reduction had been consistently underappreciated.

% Yet, saving memory is still an important topic worth studying:
% \begin{itemize}
% 	\item Multi-tenancy. A end-user laptop execute multiple programs concurrently. As an example, a typical programmer might open an IDE to edit and compile code, Zoom for remote meeting, and additionally a browser with dozens of tabs open to lookup information on the internet. Among them, the worst is the web browser, as each tab is it's own separate process, with a renderer, a rule engine, along side a JavaScript runtime. All software above consume a decent amount of ram, and contend between themselves for memory.
% 	\item Huge input. While a typical image might have a resolution of around 1000 x 1000, large images might reach a size 100 to 1000 time larger than that. Images editors and viewers typically thrash or oom upon processing images of such size. Similarly, text editors typically process text of < 1MB, and fail on logs or other large text file reaching GBs. Likewise IDEs will parse and analyze the source code of a project, then stored the analyzed results for auto completion. Upon working on huge project such IDEs will crash.
% 	\item Intermediate state. A modern computer can read and write memory at a speed of GB per seconds. Without memory reclamation technique such as garbage collection, memory will run out at a matter of seconds. However, some applications is essentially out of reach from garbage collection, and must keep most if not all intermediate states around. These applications include time travelling debugger, jupyter notebook, reverse mode automatic differentiation for deep learning, incremental computation/hash consing, and algorithm that use search, such as model checker and chess bot.
% 	\item Low memory limit. Wasm have a memory limit of 2GB, and embedded device or poor people may have device with lower ram. GPU and other accelerator use their own separate memory and usually is of smaller capacity then main memory.
% \end{itemize}

% A typical and generic solution to reduce memory consumption is by uncomputation. Uncomputation trade space for time, by tagging values with the metadata that created it(thunk), and when memory is low, deallocate values which might be used later on, recomputing them back with the thunk when the value is needed again.

% Another generic solution is swapping, where upon low memory, pages are swapped to disk and swapped back when needed again. While swapping had been implemented for basically all operating systems, it is an especially bad solution for functional and object-oriented languages, as those languages allocate and deallocate lots of small object, yet swapping work on the granularity of pages, unable to separate out the hot objects, that had been recently accessed from the code objects, that had not been accessed.

% Typically, uncomputation is implemented in an ad-hoc, case by case basis. When a software consume more memory than desired, it's programmer can (1) look at the program, (2) decide which part is using excessive memory, then (3) add custom data type to record the thunk, alongside the code to replace it (4) implement a cache eviction policy to decide what data to evict(uncompute). Another solution is to analyze the algorithm carefully, replacing the algorithm with a specialized version with recomputation in mind, see gradient checkpointing/iterative deepinging depth first search/island algorithm.

% Needless to say, this process is extremely cumbersome, taking precious developers time away from more critical task such as optimizing cpu usage or implementing new features, so a generic, automatic approach is needed to reduce memory consumption.

% \subsection{Failed Attempt}
% On first glance, uncomputation is easy, as it's cousin, lazy evaluation, is heavily studied. In lazy computation, object might be represented as thunks, which, when needed, is then computed, and the thunk is replaced by the value. A solution to uncomputation is to modify thunk, so that, upon computing, the old function is still kept. This allow the thunk to uncompute as if it had never been recomputed.

% \begin{mathpar}
% 	$Lazy Evaluation: type 'a lazy = MkLazy of ('a, unit -> 'a) either ref \\
% 	Uncomputation: type 'a uncompute = MkUncompute of 'a option ref * (unit -> 'a)$
% \end{mathpar}

% Just like lazy evaluation, we can then uniformally lift all values on top of our modified value type, inserting code that force weak head normal form upon data access, and have some cache policy to decide what to uncompute, which merely rewrite the reference back into the empty optional value.

% However, there are multiple problems with this approach:
% \begin{enumerate}
% 	\item size measurement. We want to gather statistics to decide what values to evict, and one of the most important statistics is the memory a object consume. However, the heap form a complex object graph, and fast cardinality estimation is highly complex.
% 	\item dopple ganger. The translation will lift a list into nested uncompute. Now suppose variable X hold a list, while variable Y hold a sublist of X, sharing the same representation\todo{bad wording dont know how to fix}. When X is uncomputed and recomputed, the corresponding Y part will be duplicated, so there will be two representation of Y in memory. In the extreme case, such duplication can force the program to take exponentially more memory. 
% 	\item bread crumb. A thunk capture other value as free variable, which contain a thunk part, capturing more value. This recursive process capture all value that the current value transitively compute on. Since we still need memory to store the thunk itself, and since a thunk is typically larger then an object, anything we gained on uncomputation, will be offset by the metadata.
% \end{enumerate}

% While the above three problems seems difficult, we can observe they are all but mere manifestation of recursiveness. In particular, size measurement and dopple ganger deal with recursive objects, and bread crumb deal with recursive thunk. This indicate that a solution that handle recursiveness well naturally solve the three problems at once.

% \subsection{A recursive Solution}
% To combat recursiveness we take heavy inspiration from Abstracting Abstract Machine (AAM). AAM lift abstract interpretation onto programming languages with arbitrary feature. The critical problem there is likewise recursiveness, as a naive lifting might allow the lattice infinite merge, causing the analysis to non-terminate. AAM propose to use an abstract machine that abstract over pointers, allocations and lookups to manage recursiveness.

% Likewise, the three recursiveness problem listed above can be handled by a similar manner. We started from a purely functional language, specifying it's semantic with the CEK machine. We can then abstract over pointers/allocations/lookups similarly.

% Most importantly, our key insight is that by a careful handling of our core data structure, the tock tree, which will be explained later, we can remove a value, alongside the metadata(thunk) on that value, yet still recompute it later. This solve the breadcrumb problem which render the naive apporach unsalvageable.

% Alongside our solution is proof that it is both correct and efficient. Our correctness proof state that uncomputing will not effect the final result(preservation), and will not infinite loop(progress). What get computed by the raw, uncompute-free semantic, no matter what we evicted. Our efficiency proof state that if our memory consumption is O(n), where n is the amount of live objects: objects that take O(1) time to force into weak head normal form. In particular, this imply asymptotically we use no space to store any evicted object, yet we somehow can access the metadata that recompute them!

% We had implemented our solution, alongside a eviction policy that is both fast to compute, and make better decision over classical eviction policy like LRU, random, and GDSF.

\subsection{Motivation}
Program execution consumes both space and time. While there is tons of research focused on reducing the time spent running a program, memory usage reduction has been consistently underappreciated.

Yet, saving memory is still an important topic worth studying:
\begin{itemize}
    \item Multi-tenancy. An end-user laptop executes multiple programs concurrently. As an example, a typical programmer might open an IDE to edit and compile code, Zoom for remote meetings, and additionally a browser with dozens of tabs open to look up information on the internet. Among them, the worst is the web browser, as each tab is its own separate process, with a renderer, a rule engine, and a JavaScript runtime. All software above consume a decent amount of RAM and contend between themselves for memory.
    \item Huge input. While a typical image might have a resolution of around 1000 x 1000, large images might reach a size 100 to 1000 times larger than that. Images editors and viewers typically thrash or oom upon processing images of such size. Similarly, text editors typically process text of < 1MB, and fail on logs or other large text files reaching GBs. Likewise, IDEs will parse and analyze the source code of a project and then store the analyzed results for auto-completion. Upon working on a huge project such IDEs will crash.
    \item Intermediate state. A modern computer can read and write memory at a speed of GB per second. Without memory reclamation techniques such as garbage collection, memory will run out in a matter of seconds. However, some applications are essentially out of reach from garbage collection and must keep most if not all intermediate states around. These applications include time traveling debugger, Jupyter Notebook, reverse mode automatic differentiation for deep learning, incremental computation/hash consing, and algorithms that use search, such as model checker and chess bot.
    \item Low memory limit. Wasm has a memory limit of 2GB, and embedded devices or poor people may have devices with lower RAM. GPU and other accelerator use their own separate memory and usually are of smaller capacity than main memory.
\end{itemize}

A typical and generic solution to reduce memory consumption is by uncomputation. Uncomputation trades space for time, by tagging values with the metadata that created it(thunk), and when memory is low, deallocate values that might be used later on, recomputing them back with the thunk when the value is needed again.

Another generic solution is swapping, whereupon low memory, pages are swapped to disk and swapped back when needed again. While swapping had been implemented for basically all operating systems, it is an especially bad solution for functional and object-oriented languages, as those languages allocate and deallocate lots of small objects, yet swapping work on the granularity of pages, unable to separate out the hot objects, that had been recently accessed from the code objects, that had not been accessed.

Typically, uncomputation is implemented on an ad-hoc, case-by-case basis. When software consumes more memory than desired, its programmer can (1) look at the program, (2) decide which part is using excessive memory, then (3) add custom data type to record the thunk, alongside the code to replace it (4) implement a cache eviction policy to decide what data to evict(uncompute). Another solution is to analyze the algorithm carefully, replacing the algorithm with a specialized version with recomputation in mind, see gradient checkpointing/iterative deepening depth-first search/island algorithm.

Needless to say, this process is extremely cumbersome, taking precious developers' time away from more critical tasks such as optimizing CPU usage or implementing new features, so a generic, automatic approach is needed to reduce memory consumption.

\subsection{Failed Attempt}
At first glance, uncomputation is easy, as its cousin, lazy evaluation, is heavily studied. In lazy computation, an object might be represented as thunks, which, when needed, is then computed, and the thunk is replaced by the value. A solution to uncomputation is to modify thunk, so that, upon computing, the old function is still kept. This allows the thunk to uncompute as if it had never been recomputed.

\begin{mathpar}
    $Lazy Evaluation: type 'a lazy = MkLazy of ('a, unit -> 'a) either ref \\
    Uncomputation: type 'a uncompute = MkUncompute of 'a option ref * (unit -> 'a)$
\end{mathpar}

Just like lazy evaluation, we can then uniformly lift all values on top of our modified value type, inserting code that forces weak head normal form upon data access and has some caching policy to decide what to uncompute, which merely rewrites the reference back into the empty optional value.

However, there are multiple problems with this approach:
\begin{enumerate}
    \item size measurement. We want to gather statistics to decide what values to evict, and one of the most important statistics is the memory an object consumes. However, the heap forms a complex object graph, and fast cardinality estimation is highly complex.
    \item dopple ganger. The translation will lift a list into nested uncompute. Now suppose variable X holds a list, while variable Y holds a sublist of X, sharing the same representation\todo{bad wording do not know how to fix}. When X is uncomputed and recomputed, the corresponding Y part will be duplicated, so there will be two representations of Y in memory. In the extreme case, such duplication can force the program to take exponentially more memory. 
    \item bread crumb. A thunk captures other values as a free variable, which contains a thunk part, capturing more value. This recursive process captures all values that the current value transitively computes on. Since we still need memory to store the thunk itself, and since a thunk is typically larger than an object, anything we gained on uncomputation, will be offset by the metadata.
\end{enumerate}

While the above three problems seem difficult, we can observe they are all but mere manifestations of recursiveness. In particular, size measurement and dopple ganger deal with recursive objects and breadcrumb deals with recursive thunk. This indicates that a solution that handles recursiveness will naturally solve the three problems at once.

\subsection{A recursive Solution}
To combat recursiveness we take heavy inspiration from Abstracting Abstract Machine (AAM). AAM lifts abstract interpretation onto programming languages with arbitrary features. The critical problem there is likewise recursiveness, as a naive lifting might allow the lattice infinite merge, causing the analysis to non-terminate. AAM proposes to use an abstract machine that abstracts over pointers, allocations, and lookups to manage recursiveness.

Likewise, the three recursiveness problems listed above can be handled similarly. We started from a purely functional language, specifying its semantics with the CEK machine. We can then abstract over pointers/allocations/lookups similarly.

Most importantly, our key insight is that by careful handling of our core data structure, the tock tree, which will be explained later, we can remove a value, alongside the metadata(thunk) on that value, yet still recompute it later. This solves the breadcrumb problem which renders the naive approach unsalvageable.

Alongside our solution is proof that it is both correct and efficient. Our correctness proof states that uncomputing will not affect the final result(preservation), and will not infinite loop(progress). The result will be the same as what gets computed by the raw, uncomputation-free semantic, no matter what we evicted. Our efficiency proof states that if our memory consumption is O(n), where n is the amount of live objects: objects that take O(1) time to force into weak head normal form. In particular, this implies asymptotically we use no space to store any evicted object, yet we somehow can access the metadata that recomputes them!

We implemented our solution, alongside an eviction policy that is both fast to compute and makes better decisions over classical eviction policies like LRU, random, and GDSF.
