\subsection{Motivation(1.5pg)}
\section{Intro}
Program consume memory. Program might use too much memory:
\begin{itemize}
	\item The input might be huge. Editing a file require loading the whole file into memory; Autocomplete load and parse the whole codebase; Image editors will keep multiple replica of similar images.
	\item Exploding intermediate state: Model checker and Chess bot search through a huge state space, Compiler and Interprocedural analysis expand the AST, Deep Learning/Backpropagation create and keep intermediate value. Additionally, interactive/incrmental application such as Juypter Notebook, Undo/Redo Stack, Time travelling debugger also keep intermediate state around to revert back to an older state when needed.
	\item Low memory limit. wasm have a memory limit of 2GB, embedded device or poor people may have device with lower ram. A personal computer might be running multiple programs, which must contend for a fixed memory limit. GPU and other accelerator use their own separate memory and usually is of smaller capacity then main memory.
\end{itemize}

Even when there is enough memory, it is still beneficial to use less memory. Operating System use memory for file cache, the program may have software cache, garbage collector fire less often give more free space, and extra space could be used to increase load or to execute other process.

One common-bought up solution to the memory problem is - to just buy more memory. However this only work to certain extents. Accelerators and wasm have a fixed memory limit and memory-limited algorithm can improve space consumption asymptotically, a feat that merely hardware improvement cannot catch up to. \todo{should i talk about banker paradox here?}

However, the only generic technique for reducing memory consumption, swapping, come with great drawback. Swapping move a chunk of data from main memory onto disk, and upon re-requesting it, move the data from disk back into main memory. It degrade performance significantly as disk is far slower then CPU and main memory, and it is especially bad for functional programs as most values is functional programs are boxed, they typically have less cache locality, exacerbated by swapping which operate on pages granularity.

Facing the lack of a generic solution, developers resort to ad-hoc, case-by-case solution, such as cache size tuning, reducing heap size, creating algorithm that swap and uncompute. Such solution complicate the system, and as the developer is likely not an expert on memory-constrained solution, suboptimal design/implementation is often made.

We propose uncomputation, a technique to reduce memory consumption for arbitrary purely functional programs. When needed, uncomputation select some values in the object graph according to cache policy, reverting it back into thunk, releasing memory until it is needed again, in which case it will recompute said values and replacing the thunk with the values. We provided a monadic API behind uncomputation, such that mere mechnical translation is needed to apply our work. The monadic API also grant compatibility, saving developers headache from learning and converting between a full-fledge language.

\subsection{Pebble game}
The idea of uncomputation is not new. More specifically, uncomputation had been studied rather heavily by the TCS community, by modelling it as a 'pebble game'. 

Pebble game is played on a computation graph, a directed acyclic graph where vertex represent data and edge represent computational dependency between data. The game is played with a fixed amount of identical pebble, representing memory, with each vertex possibly having one pebble on it. 

It is played with the following rules:
\begin{itemize}
\item If a vertex have no pebble, but all it's parent have pebble, a pebble can be placed on it. This represent computing a value based on it's dependency, and stroing the result.
\item A pebble can be removed from a vertex. This represent deallocating a value.
\item The game is finished if a pebble is placed on a special vertex, the result.
\end{itemize}

The aim of the game is to finish it in the minimal amount of compute step.

The pebble game is a prolific area of TCS research, with many variation of pebble game, modeling swapping or partially reversible computation, or with different cost/constraint. Special casing on the computation graph, where it form a planar graph or a tree, had likewised be studied. However, in the general case, on the classical pebble game is very diffcult.

It is hard to use less pebbles. John Hopcrof prove that for a pebble game of size X, one can use X / log(X) amount of pebble, but in general, no less. \todo{recheck this no less claim}. Moreover given a computation graph, the minimal amount of pebble (alongside the strategy) is not only NP-complete, but also inapproximable.

\todo{give an example of a pebble game.}

Our main focus is orthogonal to that of pebble games'. While the pebble game focus on finding good strategy, we focus on finding strategy effectively, using little space and time. In particular, under the lens of pebble game, the breadcrumb problem thus become 'how to store the computational graph without storing it', a seemingly impossible task.
\subsection{Guarantee(0.5pg)}
There are multiple guarantees our approach give.

Correctness guarantee: uncomputing will not effect the final result (preservation), and will not be in an infinite loop (progress). Essentially, uncomputing is transparent: the semantic is that of the same as if none happens at all.

Asymptotic guarantee: O(1) access time for all in-memory objects, and O(1) space for each in-memory objects. In particular, this imply that we are not using any space for uncomputed objects.