\section{Related Work(2pg)}
\subsection{Memory-Constrained Algorithm}
Memory Limit is a common problem. Lots of different subfields of CS had experienced such problem, and developed specialized algorithm that trade space for time, by recomputing, for their own need. \todo{cite}. In particular, a famous algorithm, TREEVERSE, had been re-discovered independently multiple times in unrelated subfields.

Keeping uncomputation at each subfield not only cause multiple re-discovery and re-implementation, wasting valuable researcher/programmer time, but also come short in integration. Suppose a complex, memory-hungry software have 2 sub-parts, both of which are memory hungry. How much uncompute responsibility should each part take? What happens if two part depend on each other?

Zombie not only provide an automatic method for memory-constraint algorithm, it also solve the constitutionality problem: by using a single central cache policy, responsibility is implicitly calculated and spreaded to the two subcomponent. Furthermore, by using a recursive cache policy, the union find, the dependency between the system is also taken into consideration.
\subsection{Pebbling}
\todo{we had talked about pebbling in intro. delete?}
Space time tradeoff is a known-topic in Theoretical Computer Science. It is mostly modeled as a "pebble game" \todo{quickly explain}. It had been known that this problem is NP-complete and inapproximable. There are also generic result in this space. Hopcroft prove that for any program that take N time, it could be modified to take O(N / log(N)) space. However, the proof is non-constructive, and thus does not help building an automatic runtime like ours.

Our work mostly focus on the overhead problem - how to minimize the overhead of recording metadata needed for recomputation. How to make an API that is generic yet embeddable. In another words - our work is orthgonal to advances in pebble game. In fact, one could imagine replacing our cache policy with some of those algorithms.

We also provide a greedy cache policy that is both fast and make reasonably good decision on the list of benchmark, both real and synthetic, provided below, since the theoretical result is weak and non-constructive. There are some specialized result of pebble game on planar graph and on tree. One could imagine upon detecting planar graph/tree, switching from our cache policy to the theoretically optimal ones.
\subsection{Memoization/Caching}
Uncomputation is the dual to Memoization. While memoization and caching use extra space to store computational result to avoid recomputing, we are unstoring result and recomputing them to save memory.
\subsection{Garbage Collection}
Garbage Collection is known to ocassionally suffer memory leak, when a live object hold reference to object no-longer used, as GC work by tracing pointers. There are some work that try to use liveness analysis to solve this problem, by detecting when a object is no longer used yet still pointed-to, then removing it. Needless to say, such approach will always be limited by rice theorem and there will always be garbage it cannot collect.

Zombie, when seen as a GC technique, offer an interesting approach to this problem. While garbage collector ask for permission to remove an object, instead zombie ask for forgiveness - as a value live longer and longer untouched, it will be more and more profitable for the cache policy to uncompute it, and once uncomputed, if it is dead object, it will not be used again, hence we sucessifully deallocate the memory. If our guess is wrong, however, we pay the price of recomputing when needed again. This distinction share resemblence to optimistic lock vs pessmistic lock.
\subsection{Compression}
Uncomputation is, fundamentally speaking, a form of compression.

Pigeon hole principle mean it is fundamentally impossible to have a generic compression algorithm. Uncomputation exploit the inductive bias that a program pointer is smaller then the memory consumption objects represent.
